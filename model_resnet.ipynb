{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49bf253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π ===\n",
    "with open(\"eccv_18_annotation_files/train_annotations.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# === 2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è category_id ‚Üí class_index ===\n",
    "# üîπ –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –∫–ª–∞—Å—Å—ã\n",
    "selected_class_names = [\"deer\", \"raccoon\", \"coyote\", \"dog\"]\n",
    "\n",
    "# üîπ –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤ —Å –∏—Ö –∏—Å—Ö–æ–¥–Ω—ã–º–∏ id\n",
    "all_categories = data[\"categories\"]\n",
    "name_to_id = {c[\"name\"]: c[\"id\"] for c in all_categories}\n",
    "selected_class_ids = [name_to_id[name] for name in selected_class_names]\n",
    "\n",
    "# üîπ –ù–æ–≤—ã–π –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥: old_id ‚Üí new_index\n",
    "id_to_idx = {cid: idx for idx, cid in enumerate(selected_class_ids)}\n",
    "\n",
    "# üîπ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø–æ –Ω—É–∂–Ω—ã–º –∫–ª–∞—Å—Å–∞–º\n",
    "filtered_annotations = [ann for ann in data[\"annotations\"] if ann[\"category_id\"] in selected_class_ids]\n",
    "\n",
    "# üîπ –û—Ç–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≥–¥–µ –µ—Å—Ç—å —ç—Ç–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "used_image_ids = set(ann[\"image_id\"] for ann in filtered_annotations)\n",
    "filtered_images = [img for img in data[\"images\"] if img[\"id\"] in used_image_ids]\n",
    "\n",
    "# üîπ –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 100 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "filtered_images = filtered_images[:100]\n",
    "\n",
    "# üîπ –ü–µ—Ä–µ—Ñ–∏–ª—å—Ç—Ä—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –ø–æ —ç—Ç–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º\n",
    "selected_image_ids = set(img[\"id\"] for img in filtered_images)\n",
    "filtered_annotations = [ann for ann in filtered_annotations if ann[\"image_id\"] in selected_image_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af97cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. –ö–ª–∞—Å—Å Dataset ===\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, images, annotations, image_dir, category_id_mapping, transforms=None):\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "        self.image_dir = image_dir\n",
    "        self.category_id_mapping = category_id_mapping\n",
    "\n",
    "        self.image_id_to_annotations = {}\n",
    "        for ann in annotations:\n",
    "            image_id = ann.get(\"image_id\")\n",
    "            if image_id is not None:\n",
    "                self.image_id_to_annotations.setdefault(image_id, []).append(ann)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_info = self.images[idx]\n",
    "        image_id = image_info[\"id\"]\n",
    "        file_name = image_info[\"file_name\"]\n",
    "        image_path = os.path.join(self.image_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}\") from e\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in self.image_id_to_annotations.get(image_id, []):\n",
    "            if \"bbox\" not in ann or \"category_id\" not in ann:\n",
    "                continue\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "            class_id = self.category_id_mapping.get(ann[\"category_id\"])\n",
    "            if class_id is None:\n",
    "                continue\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(class_id)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = [[0, 0, 1, 1]]\n",
    "            labels = [0]\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            image = F.to_tensor(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943f3f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:44<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π Loss: 16.8335\n",
      "\n",
      "üß† Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:43<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π Loss: 18.1189\n",
      "\n",
      "üß† Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:43<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π Loss: 16.8346\n",
      "\n",
      "üß† Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:45<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4 –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π Loss: 17.0745\n",
      "\n",
      "üß† Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:45<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5 –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π Loss: 17.2626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏ ===\n",
    "image_dir = \"eccv_18_all_images_sm/eccv_18_all_images_sm\"  # –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "dataset = AnimalDataset(\n",
    "    filtered_images,\n",
    "    filtered_annotations,\n",
    "    image_dir=image_dir,\n",
    "    category_id_mapping=id_to_idx\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "num_classes = len(id_to_idx) + 1  # +1 –¥–ª—è background\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0, momentum=0.9)\n",
    "\n",
    "# === 5. –û–±—É—á–µ–Ω–∏–µ —Å tqdm ===\n",
    "model.train()\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"\\nüß† Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch + 1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ä–µ–¥–Ω–∏–π Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb20671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# === idx_to_name: —Å–ª–æ–≤–∞—Ä—å –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ (—Å–æ–∑–¥–∞–Ω —Ä–∞–Ω–µ–µ) ===\n",
    "# –ü—Ä–∏–º–µ—Ä: idx_to_name = {0: \"deer\", 1: \"raccoon\", 2: \"coyote\", 3: \"dog\"}\n",
    "\n",
    "def detect_on_video(video_path, model, device, idx_to_name, output_path=\"output.avi\", conf_thresh=0.5):\n",
    "    model.eval()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞–¥—Ä –≤ PIL ‚Üí Tensor\n",
    "            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "            predictions = model(image_tensor)[0]\n",
    "\n",
    "            for box, label, score in zip(predictions['boxes'], predictions['labels'], predictions['scores']):\n",
    "                if score < conf_thresh:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box.tolist())\n",
    "                class_name = idx_to_name.get(label.item(), \"unknown\")\n",
    "                color = (0, 255, 0)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                text = f\"{class_name}: {score:.2f}\"\n",
    "                cv2.putText(frame, text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"üé¨ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_on_video(\n",
    "    video_path=\"your_input_video.mp4\",\n",
    "    model=model,\n",
    "    device=device,\n",
    "    idx_to_name=idx_to_name,\n",
    "    output_path=\"detection_output.avi\",\n",
    "    conf_thresh=0.5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
